from optimum.onnxruntime import ORTModelForFeatureExtraction
from transformers import AutoTokenizer
import os

model_id = "IEITYuan/Yuan-embedding-2.0-en"
save_dir = "backend/models/yuan-onnx-trt"
os.makedirs(save_dir, exist_ok=True)

# ðŸ’¡â€¯Make sure TensorRT DLLs are visible
venv = os.environ.get("VIRTUAL_ENV", "")
trt_libs = os.path.join(venv, "Lib", "site-packages", "tensorrt_libs")
os.environ["PATH"] = f"{trt_libs};{os.environ['PATH']}"

print("ðŸš€â€¯Exportingâ€¯ONNXâ€¯modelâ€¯forâ€¯TensorRT...")
model = ORTModelForFeatureExtraction.from_pretrained(
    model_id,
    export=True,
    providers=[
        "TensorrtExecutionProvider",
        "CUDAExecutionProvider",
        "CPUExecutionProvider"
    ],
    trust_remote_code=True,
    dtype="float16",          # FP16â€¯â†’â€¯fastestâ€¯forâ€¯TensorRT
    opset=18,                # Opsetâ€¯18â€¯isâ€¯widelyâ€¯supported
)
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print(f"âœ… TensorRT-ready ONNX model saved to {save_dir}")